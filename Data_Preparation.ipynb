{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44f06023-772d-4ab9-8e85-ce344fee0419",
   "metadata": {
    "tags": []
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0c8afca-7f8f-4f34-92d7-92a07ccddb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, math, csv\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import tifffile\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f383ac7-8ff6-4cd8-9226-b9292a6383c2",
   "metadata": {},
   "source": [
    "# Define file paths and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "576694dd-5919-4f49-a731-737554688338",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tif_image_stack_paths = [\n",
    "    './row_data/On_time_varied_data/01_25LP_532_100ms_1nMRO_1d8nt_1nMP3_trolox.ome.tif',\n",
    "    './row_data/On_time_varied_data/01_25LP_532_100ms_400pMRO_1d10nt_1nMP3_trolox.tif',\n",
    "    './row_data/On_time_varied_data/Another_dataset/01_13LP_532_100ms_1nMRO_1d8nt_1nMP3_trolox-002.tif',\n",
    "    './row_data/On_time_varied_data/Another_dataset/02_13LP_532_100ms_200pMRO_1d10nt_1nMP3_trolox-001.tif'\n",
    "]\n",
    "all_drift_corrected_paths = [\n",
    "    './row_data/On_time_varied_data/01_25LP_532_100ms_1nMRO_1d8nt_1nMP3_trolox_locs_render_picked.hdf5',\n",
    "    './row_data/On_time_varied_data/01_25LP_532_100ms_400pMRO_1d10nt_1nMP3_trolox_locs_render_picked.hdf5',\n",
    "    './row_data/On_time_varied_data/Another_dataset/01_13LP_532_100ms_1nMRO_1d8nt_1nMP3_trolox_locs_render1000_picked.hdf5',\n",
    "    './row_data/On_time_varied_data/Another_dataset/02_13LP_532_100ms_200pMRO_1d10nt_1nMP3_trolox_locs_render1000_picked.hdf5'\n",
    "]\n",
    "all_drift_uncorrected_paths = [\n",
    "    './row_data/On_time_varied_data/Not_drift_corrected/01_25LP_532_100ms_1nMRO_1d8nt_1nMP3_trolox_locs.hdf5',\n",
    "    './row_data/On_time_varied_data/Not_drift_corrected/01_25LP_532_100ms_400pMRO_1d10nt_1nMP3_trolox_locs.hdf5',\n",
    "    './row_data/On_time_varied_data/Another_dataset/Not_drift_corrected/01_13LP_532_100ms_1nMRO_1d8nt_1nMP3_trolox_locs.hdf5',\n",
    "    './row_data/On_time_varied_data/Another_dataset/Not_drift_corrected/02_13LP_532_100ms_200pMRO_1d10nt_1nMP3_trolox_locs.hdf5'\n",
    "    ]\n",
    "all_drift_trajectory_paths = [\n",
    "    './row_data/On_time_varied_data/01_25LP_532_100ms_1nMRO_1d8nt_1nMP3_trolox_locs_240229_094939_drift.txt',\n",
    "    './row_data/On_time_varied_data/01_25LP_532_100ms_400pMRO_1d10nt_1nMP3_trolox_locs_240229_094701_drift.txt',\n",
    "    './row_data/On_time_varied_data/Another_dataset/01_13LP_532_100ms_1nMRO_1d8nt_1nMP3_trolox_locs_240303_101726_drift.txt',\n",
    "    './row_data/On_time_varied_data/Another_dataset/02_13LP_532_100ms_200pMRO_1d10nt_1nMP3_trolox_locs_240303_102117_drift.txt'\n",
    "    ]\n",
    "data_dir = './data'\n",
    "box_width = 10\n",
    "max_drift_distance = box_width/5 # Unit: Pixel, assume that drift speed is less than 130 um/100ms\n",
    "# Our setting: pixel height/width: 6.5 um, frame rate = 10 Hz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2784e4f0-cda8-45af-a452-a9486af01f01",
   "metadata": {},
   "source": [
    "# Load and save molecular localization and drift data to .csv tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074772ca-1114-4748-b56d-393b100a9b27",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for domain_idx, hdf5_path in enumerate(all_drift_corrected_paths):\n",
    "    save_path = f'{os.path.dirname(hdf5_path)}/domain_{domain_idx + 1}.csv'\n",
    "    print(save_path)\n",
    "    if not os.path.exists(save_path) or reprocess:\n",
    "        f = h5py.File(hdf5_path)\n",
    "        df = pd.DataFrame(np.array(f['locs']))\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print('Domain:', domain_idx + 1, '\\n', df)\n",
    "        \n",
    "for domain_idx, hdf5_path in enumerate(all_drift_uncorrected_paths):\n",
    "    save_path = f'{os.path.dirname(hdf5_path)}/domain_{domain_idx + 1}.csv'\n",
    "    print(save_path)\n",
    "    if not os.path.exists(save_path) or reprocess:\n",
    "        f = h5py.File(hdf5_path)\n",
    "        df = pd.DataFrame(np.array(f['locs']))\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print('Domain:', domain_idx + 1, '\\n', df)\n",
    "\n",
    "for domain_idx, text_path in enumerate(all_drift_trajectory_paths):\n",
    "    if len(text_path) > 0:\n",
    "        save_path = f'{os.path.dirname(text_path)}/drift_{domain_idx + 1}.csv'\n",
    "        if not os.path.exists(save_path) or reprocess:\n",
    "            # Read the text file and skip blank lines\n",
    "            with open(text_path, 'r') as file:\n",
    "                lines = [line.strip() for line in file if line.strip() and not line.startswith('#')]\n",
    "\n",
    "            # Create a list to store data\n",
    "            data = []\n",
    "\n",
    "            # Iterate through lines and extract values\n",
    "            for i, line in enumerate(lines):\n",
    "                values = line.split()\n",
    "                dx = float(values[0])\n",
    "                dy = float(values[1])\n",
    "                data.append([i, dx, dy])\n",
    "\n",
    "            # Create a DataFrame\n",
    "            df = pd.DataFrame(data, columns=['frame', 'dx', 'dy'])\n",
    "            df.to_csv(save_path, index=False)\n",
    "            print('Domain:', domain_idx + 1, '\\n', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412f3573-395e-48ae-a04d-56ddbda6fa9d",
   "metadata": {},
   "source": [
    "# Substract the mean intensity for each frame, mask and save original fluorescent spots in tif format (use uncorrected df for a complete collection of binding events and blinking correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120659b5-9a05-4ac9-978d-990419d40cd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate Euclidean distance\n",
    "def calculate_distance(x1, y1, x2, y2):\n",
    "    return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "\n",
    "def save_image_patches(original_frame, image, x, y, box_width, patch_save_path, subtracted_patch_save_path, mean_background_intensity_df):\n",
    "    # Calculate box boundaries\n",
    "    box_left = max(0, round(x - box_width / 2))\n",
    "    box_right = min(image.shape[1], round(x + box_width / 2))\n",
    "    box_top = max(0, round(y - box_width / 2))\n",
    "    box_bottom = min(image.shape[0], round(y + box_width / 2))\n",
    "\n",
    "    # Extract image patch\n",
    "    image_patch = image[box_top:box_bottom, box_left:box_right]\n",
    "    \n",
    "    # Save the image patch\n",
    "    os.makedirs(os.path.dirname(patch_save_path), exist_ok=True)\n",
    "    image_patch_uint16 = image_patch.astype(np.uint16)\n",
    "    # print(f\"image_patch_uint16: {image_patch_uint16}\")\n",
    "    image_patch_pil = Image.fromarray(image_patch_uint16)\n",
    "    image_patch_pil.save(patch_save_path)\n",
    "\n",
    "    # subtract the mean background intensity from the image patch\n",
    "    image_patch_mean = np.mean(image_patch_uint16)\n",
    "    background_mean_intensity = mean_background_intensity_df[mean_background_intensity_df['frame']==original_frame+1]['mean_background_intensity'].values\n",
    "    masked_image_patch_uint16 = image_patch_uint16\n",
    "    # print(original_frame, background_mean_intensity)\n",
    "    background_mean_intensity = background_mean_intensity.item()\n",
    "    mask_threshold = (image_patch_mean + background_mean_intensity) / 2\n",
    "    background_offset = 100\n",
    "    masked_image_patch_uint16[masked_image_patch_uint16 < mask_threshold] = background_offset + background_mean_intensity\n",
    "    os.makedirs(os.path.dirname(subtracted_patch_save_path), exist_ok=True)\n",
    "    subtracted_image_patch_uint16 = masked_image_patch_uint16 - background_mean_intensity\n",
    "    # print(f\"subtracted_image_patch_uint16: {subtracted_image_patch_uint16}\")                    \n",
    "    subtracted_image_patch_pil = Image.fromarray(subtracted_image_patch_uint16)\n",
    "    subtracted_image_patch_pil.save(subtracted_patch_save_path)\n",
    "\n",
    "    # For debug\n",
    "    # image = Image.open(subtracted_patch_save_path)\n",
    "    # # Convert the image to a numpy array\n",
    "    # image_array = np.array(image)\n",
    "    # print(f\"image_array: {image_array}\") # the same as image_patch/subtracted_image_patch_uint16\n",
    "    # print(stop)\n",
    "\n",
    "process_domain = 1\n",
    "blinking_correction_th = 10\n",
    "read_exist, reprocess = False, True\n",
    "mean_background_intensity_df = pd.DataFrame()\n",
    "# Loop through each TIFF file path\n",
    "for domain_idx, tif_path in enumerate(all_tif_image_stack_paths):\n",
    "    if domain_idx in [process_domain - 1]: # need to minus 1\n",
    "        mean_background_intensity_file = f\"{data_dir}/spots/domain_{domain_idx + 1}/mean_background_intensity.csv\"\n",
    "        # Read TIFF image stack\n",
    "        image_stack = tifffile.imread(tif_path)\n",
    "\n",
    "        # Read original localizations without drift corrections\n",
    "        locs_path = f'{os.path.dirname(tif_path)}/Not_drift_corrected/domain_{domain_idx + 1}.csv'\n",
    "        locs_df = pd.read_csv(locs_path)\n",
    "\n",
    "        # Read drift trajectories\n",
    "        drift_path = f'{os.path.dirname(tif_path)}/drift_{domain_idx + 1}.csv'\n",
    "        if not os.path.exists(drift_path):\n",
    "            continue\n",
    "        drift_df = pd.read_csv(drift_path)\n",
    "\n",
    "        # Print information about the read data\n",
    "        print(f\"TIFF Path: {tif_path}\")\n",
    "        print(f\"Shape of Image Stack: {image_stack.shape}\")\n",
    "        print(f\"Data Type of Image Stack: {image_stack.dtype}\")\n",
    "\n",
    "        # Convert image stack to a supported data type (e.g., float32)\n",
    "        image_stack = image_stack.astype(np.float32)\n",
    "\n",
    "        corrected_save_path = f'{os.path.dirname(tif_path)}/corrected_{domain_idx + 1}.csv'\n",
    "        if not os.path.exists(corrected_save_path) or reprocess:\n",
    "            # Merge locs_df and drift_df on \"frame\"\n",
    "            merged_df = pd.merge(locs_df, drift_df, on='frame', suffixes=('', '_drift'))\n",
    "\n",
    "            # Subtract \"dx\" from \"x\" and \"dy\" from \"y\"\n",
    "            merged_df['x_corrected'] = merged_df['x'] - merged_df['dx']\n",
    "            merged_df['y_corrected'] = merged_df['y'] - merged_df['dy']\n",
    "\n",
    "            # Reset the index within each group\n",
    "            merged_df['spot'] = merged_df.groupby('frame').cumcount()\n",
    "\n",
    "            # Drop unnecessary columns\n",
    "            merged_df.to_csv(corrected_save_path, index=False)\n",
    "\n",
    "        linked_save_path = f\"{data_dir}/spots/domain_{domain_idx + 1}_indexes_linked.csv\"\n",
    "        if os.path.exists(linked_save_path) and read_exist:\n",
    "            indexes_df = pd.read_csv(linked_save_path)\n",
    "        else:\n",
    "            corrected_df = pd.read_csv(corrected_save_path) # [x_corrected, y_corrected] can replace groups as binding site labels\n",
    "            indexes_df = corrected_df\n",
    "            # Initialize\n",
    "            indexes_df['group'] = np.nan\n",
    "            indexes_df['x_group'] = np.nan\n",
    "            indexes_df['y_group'] = np.nan\n",
    "            indexes_df['missed_from_frame'] = np.nan\n",
    "            indexes_df['missed_from_spot'] = np.nan\n",
    "        indexes_df_grouped_by_frame = indexes_df.groupby('frame')\n",
    "        \n",
    "        # Read original localizations with drift corrections\n",
    "        grouped_path = f'{os.path.dirname(tif_path)}/domain_{domain_idx + 1}.csv'\n",
    "        grouped_df = pd.read_csv(grouped_path)\n",
    "\n",
    "        # Extract light spots from frames\n",
    "        total_steps = len(grouped_df)\n",
    "        grouped_df = grouped_df.sort_values(by='frame')\n",
    "        for grouped_idx, row in tqdm(grouped_df.iterrows(), total=total_steps):\n",
    "            # if pd.isna(row['spot']):\n",
    "            frame, group, group_x, group_y = int(row['frame']), row['group'], row['x'], row['y']\n",
    "            image = image_stack[frame]\n",
    "            \n",
    "            image_save_path = f\"{data_dir}/spots/domain_{domain_idx + 1}/images/frame_{frame}.png\"\n",
    "            background_image_save_path = f\"{data_dir}/spots/domain_{domain_idx + 1}/images_background/frame_{frame}.tif\"\n",
    "            if not os.path.exists(image_save_path) or reprocess:\n",
    "                fig1, ax1 = plt.subplots()\n",
    "                ax1.imshow(image)\n",
    "                \n",
    "            # Save background tiff images\n",
    "            os.makedirs(os.path.dirname(background_image_save_path), exist_ok=True)\n",
    "            if reprocess or not os.path.exists(mean_background_intensity_file):\n",
    "                # Initialize mean_background_intensity_df with the necessary columns\n",
    "                mean_background_intensity_df = pd.DataFrame(columns=['domain', 'frame', 'mean_background_intensity'])\n",
    "                mean_background_intensity_df.to_csv(mean_background_intensity_file, index=False)\n",
    "            else:\n",
    "                mean_background_intensity_df = pd.read_csv(mean_background_intensity_file)\n",
    "            for frame_idx in np.arange(frame-blinking_correction_th, frame+1):\n",
    "                if frame_idx >= 0 and (frame_idx+1 not in mean_background_intensity_df['frame'].values):\n",
    "                    one_frame_locs_df = indexes_df_grouped_by_frame.get_group(frame_idx)\n",
    "                    # print(f'Processing background for frame {frame_idx}')\n",
    "                    # Calculate the background mean intensity\n",
    "                    background_mask = np.ones_like(image)\n",
    "                    for _, row in one_frame_locs_df.iterrows():\n",
    "                        spot, x, y, x_corrected, y_corrected = int(row['spot']), row['x'], row['y'], row['x_corrected'], row['y_corrected']\n",
    "                        # Calculate box boundaries\n",
    "                        box_left = max(0, round(x - box_width / 2))\n",
    "                        box_right = min(image.shape[1], round(x + box_width / 2))\n",
    "                        box_top = max(0, round(y - box_width / 2))\n",
    "                        box_bottom = min(image.shape[0], round(y + box_width / 2))\n",
    "        \n",
    "                        # Exclude area with light spots\n",
    "                        background_mask[box_top:box_bottom, box_left:box_right] = 0\n",
    "        \n",
    "                    # Calculate the average background intensity of the image_patch\n",
    "                    # mean_background_intensity = image * background_mask / np.sum(background_mask)\n",
    "                    \n",
    "                    # Set values smaller than the mean intensity to 0\n",
    "                    # print(background_image_save_path)\n",
    "                    background_image = image * background_mask\n",
    "                    # print(f\"background_image: {background_image}\")\n",
    "                    background_image_uint16 = background_image.astype(np.uint16)\n",
    "                    # print(f\"background_image_uint16: {background_image_uint16}\")\n",
    "                    # fig, ax = plt.subplots()\n",
    "                    background_image_pil = Image.fromarray(background_image_uint16)\n",
    "                    # ax.axis('off')\n",
    "                    # ax.imshow(background_image) # , interpolation='none'\n",
    "                    # fig.savefig(background_image_save_path, bbox_inches='tight', pad_inches=0)\n",
    "                    background_image_pil.save(background_image_save_path)\n",
    "                    background_mean_intensity = np.sum(background_image_uint16) / np.sum(background_mask)\n",
    "                    # print(f\"background_mean_intensity: {background_mean_intensity}\")\n",
    "                    current_df_len = len(mean_background_intensity_df)\n",
    "                    mean_background_intensity_df.loc[current_df_len, 'domain'] = domain_idx + 1\n",
    "                    mean_background_intensity_df.loc[current_df_len, 'frame'] = frame_idx + 1\n",
    "                    mean_background_intensity_df.loc[current_df_len, 'mean_background_intensity'] = background_mean_intensity\n",
    "                    mean_background_intensity_df.to_csv(mean_background_intensity_file, index=False)\n",
    "                    # print(frame_idx+1, mean_background_intensity_df[mean_background_intensity_df['frame']==frame_idx+1]['frame'].values.item())\n",
    "                    \n",
    "                    # For debug\n",
    "                    # image_read = Image.open(background_image_save_path)\n",
    "                    # image_read_array = np.array(image_read)  # Convert back to original scale\n",
    "                    # print(f\"image_read_array: {image_read_array}\") # the same as background_image\n",
    "                    # print(stop)\n",
    "\n",
    "            existing_groups_in_linked_frames = {}\n",
    "            for frame_idx in np.arange(frame, frame-1-blinking_correction_th, -1):\n",
    "                if frame_idx < 0:\n",
    "                    continue\n",
    "                one_frame_locs_df = indexes_df_grouped_by_frame.get_group(frame_idx)\n",
    "                existing_groups_in_linked_frames[f'frame_{frame_idx}'] = one_frame_locs_df['group'].values.tolist()\n",
    "                if group not in existing_groups_in_linked_frames[f'frame_{frame_idx}']:\n",
    "                    min_distance = max_drift_distance\n",
    "                    for corrected_idx, row in one_frame_locs_df.iterrows():\n",
    "                        original_frame, spot, x, y, x_corrected, y_corrected = int(row['frame']), int(row['spot']), row['x'], row['y'], row['x_corrected'], row['y_corrected']\n",
    "                        patch_save_path = f\"{data_dir}/spots/domain_{domain_idx + 1}/patches/frame_{frame + 1}/spot_{spot + 1}.tif\"\n",
    "                        subtracted_patch_save_path = f\"{data_dir}/spots/domain_{domain_idx + 1}/patches_subtracted/frame_{frame + 1}/spot_{spot + 1}.tif\"\n",
    "                        \n",
    "                        if not os.path.exists(subtracted_patch_save_path) or reprocess:\n",
    "                            save_image_patches(original_frame, image, x, y, box_width, patch_save_path, subtracted_patch_save_path, mean_background_intensity_df)\n",
    "                            \n",
    "                        if not os.path.exists(image_save_path) or reprocess:\n",
    "                            # save the image with localizations using matplotlib\n",
    "                            box = [y - box_width/2, x - box_width/2, y + box_width/2, x + box_width/2]\n",
    "                            rect = plt.Rectangle((box[1], box[0]), box[3] - box[1], box[2] - box[0],\n",
    "                                                 linewidth=1, edgecolor='g', facecolor='none')\n",
    "                            ax1.add_patch(rect)\n",
    "                            ax1.text((box[1] + box[3]) // 2, box[0], str(spot), color='r', fontsize=8)\n",
    "                            ax1.plot(x, y, 'ro', markersize=1)\n",
    "        \n",
    "                        distance = calculate_distance(x_corrected, y_corrected, group_x, group_y)\n",
    "                        group_value = indexes_df.loc[corrected_idx, 'group']\n",
    "                        # print(distance < min_distance, np.isnan(group_value), (group not in existing_groups_in_linked_frames[f'frame_{frame_idx}']))\n",
    "                        # if distance < min_distance:\n",
    "                        #     print(f'distance: {distance}, min_distance: {min_distance}, group_value: {group_value}')\n",
    "                        if distance < min_distance and np.isnan(group_value) and (group not in existing_groups_in_linked_frames[f'frame_{frame_idx}']):\n",
    "                            min_distance = distance\n",
    "                            indexes_df.loc[corrected_idx, 'group'] = group\n",
    "                            indexes_df.loc[corrected_idx, 'x_group'] = x_corrected\n",
    "                            indexes_df.loc[corrected_idx, 'y_group'] = y_corrected\n",
    "                            existing_groups_in_linked_frames[f'frame_{frame_idx}'].append(group)\n",
    "                            if original_frame < frame - 1:\n",
    "                                for missed_frame_idx in np.arange(original_frame + 1, frame):\n",
    "                                    if group not in existing_groups_in_linked_frames[f'frame_{missed_frame_idx}']:\n",
    "                                        missed_patch_save_path = f\"{data_dir}/spots/domain_{domain_idx + 1}/patches/frame_{missed_frame_idx + 1}/missed_frame_{frame + 1}_spot_{spot + 1}.tif\"\n",
    "                                        missed_subtracted_patch_save_path = f\"{data_dir}/spots/domain_{domain_idx + 1}/patches_subtracted/frame_{missed_frame_idx + 1}/missed_frame_{frame + 1}_spot_{spot + 1}.tif\"\n",
    "                                        missed_frame_drift_row = drift_df[drift_df['frame']==missed_frame_idx]\n",
    "                                        dx, dy = missed_frame_drift_row.dx.item(), missed_frame_drift_row.dy.item()\n",
    "                                        x_missed, y_missed = x_corrected - dx, y_corrected - dy\n",
    "                                        missed_image = image_stack[missed_frame_idx]\n",
    "                                        # Add a new empty row \n",
    "                                        current_len = len(indexes_df)\n",
    "                                        indexes_df.loc[current_len] = [np.nan] * len(indexes_df.columns)\n",
    "                                        indexes_df.loc[current_len, 'frame'] = missed_frame_idx\n",
    "                                        indexes_df.loc[current_len, 'group'] = group\n",
    "                                        indexes_df.loc[current_len, 'spot'] = -1 # undetected spots\n",
    "                                        indexes_df.loc[current_len, 'x'] = x_missed\n",
    "                                        indexes_df.loc[current_len, 'y'] = y_missed\n",
    "                                        indexes_df.loc[current_len, 'dx'] = dx\n",
    "                                        indexes_df.loc[current_len, 'dy'] = dy\n",
    "                                        indexes_df.loc[current_len, 'x_group'] = x_corrected\n",
    "                                        indexes_df.loc[current_len, 'y_group'] = y_corrected\n",
    "                                        indexes_df.loc[current_len, 'missed_from_frame'] = frame\n",
    "                                        indexes_df.loc[current_len, 'missed_from_spot'] = spot\n",
    "                                        \n",
    "                                        if not os.path.exists(missed_patch_save_path) or not os.path.exists(missed_subtracted_patch_save_path):\n",
    "                                            save_image_patches(original_frame, missed_image, x_missed, y_missed, box_width, missed_patch_save_path, missed_subtracted_patch_save_path, mean_background_intensity_df)\n",
    "                                            print(f'Saving missing patches in frame {missed_frame_idx + 1} for frame {frame + 1} group {group} spot {spot + 1}')\n",
    "\n",
    "            if not os.path.exists(image_save_path) or reprocess:\n",
    "                plt.title(f'Domain {domain_idx + 1} Frame {frame + 1} with bounding boxes and spot indexes')\n",
    "                os.makedirs(os.path.dirname(image_save_path), exist_ok=True)\n",
    "                plt.savefig(image_save_path)\n",
    "                # Clear the contents of ax\n",
    "                ax1.cla()\n",
    "                # Close the entire figure\n",
    "                plt.close(fig1)\n",
    "\n",
    "            if grouped_idx % 10000 == 0:\n",
    "                indexes_df.to_csv(linked_save_path, index=False)\n",
    "\n",
    "        indexes_df.to_csv(linked_save_path, index=False)  \n",
    "        print(\"=\"*50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
